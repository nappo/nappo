
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>nappo.core.algos package &#8212; nappo 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="nappo.core.envs package" href="nappo.core.envs.html" />
    <link rel="prev" title="nappo.core package" href="nappo.core.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="nappo.core.envs.html" title="nappo.core.envs package"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="nappo.core.html" title="nappo.core package"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">nappo 0.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="nappo.html" >nappo package</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="nappo.core.html" accesskey="U">nappo.core package</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="nappo-core-algos-package">
<h1>nappo.core.algos package<a class="headerlink" href="#nappo-core-algos-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-nappo.core.algos.base">
<span id="nappo-core-algos-base-module"></span><h2>nappo.core.algos.base module<a class="headerlink" href="#module-nappo.core.algos.base" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nappo.core.algos.base.Algo">
<em class="property">class </em><code class="sig-prename descclassname">nappo.core.algos.base.</code><code class="sig-name descname">Algo</code><a class="headerlink" href="#nappo.core.algos.base.Algo" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Base class for all algorithms</p>
<dl class="py method">
<dt id="nappo.core.algos.base.Algo.acting_step">
<em class="property">abstract </em><code class="sig-name descname">acting_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obs</span></em>, <em class="sig-param"><span class="n">rhs</span></em>, <em class="sig-param"><span class="n">done</span></em>, <em class="sig-param"><span class="n">deterministic</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.acting_step" title="Permalink to this definition">¶</a></dt>
<dd><p>PPO acting function.</p>
<dl class="simple">
<dt>obs: torch.tensor</dt><dd><p>Current world observation</p>
</dd>
<dt>rhs: torch.tensor</dt><dd><p>RNN recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p>
</dd>
<dt>done: torch.tensor</dt><dd><p>1.0 if current obs is the last one in the episode, else 0.0.</p>
</dd>
<dt>deterministic: bool</dt><dd><p>Whether to randomly sample action from predicted distribution or take the mode.</p>
</dd>
</dl>
<dl class="simple">
<dt>action: torch.tensor</dt><dd><p>Predicted next action.</p>
</dd>
<dt>clipped_action: torch.tensor</dt><dd><p>Predicted next action (clipped to be within action space).</p>
</dd>
<dt>rhs: torch.tensor</dt><dd><p>Policy recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p>
</dd>
<dt>other: dict</dt><dd><p>Additional PPO predictions, value score and action log probability,
which are not used in other algorithms.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.base.Algo.algo_factory">
<em class="property">abstract classmethod </em><code class="sig-name descname">algo_factory</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.algo_factory" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a function to create new Algo instances</p>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.base.Algo.apply_gradients">
<em class="property">abstract </em><code class="sig-name descname">apply_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gradients</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.apply_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Take an optimization step, previously setting new gradients if provided.</p>
<dl class="simple">
<dt>gradients: list of tensors</dt><dd><p>List of actor_critic gradients.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.base.Algo.compute_gradients">
<em class="property">abstract </em><code class="sig-name descname">compute_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">grads_to_cpu</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.compute_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute loss and compute gradients but don’t do optimization step,
return gradients instead.</p>
<dl class="simple">
<dt>data: dict</dt><dd><p>data batch containing all required tensors to compute PPO loss.</p>
</dd>
<dt>grads_to_cpu: bool</dt><dd><p>If gradient tensor will be sent to another node, need to be in CPU.</p>
</dd>
</dl>
<dl class="simple">
<dt>grads: list of tensors</dt><dd><p>List of actor_critic gradients.</p>
</dd>
<dt>info: dict</dt><dd><p>Dict containing current PPO iteration information.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.base.Algo.set_weights">
<em class="property">abstract </em><code class="sig-name descname">set_weights</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.set_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Update actor critic with the given weights</p>
<dl class="simple">
<dt>weights: dict of tensors</dt><dd><p>Dict containing actor_critic weights to be set.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.base.Algo.update_algo_parameter">
<em class="property">abstract </em><code class="sig-name descname">update_algo_parameter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parameter_name</span></em>, <em class="sig-param"><span class="n">new_parameter_value</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.update_algo_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>parameter_name</cite> is an attribute of the algorithm, change its value
to <cite>new_parameter_value value</cite>.</p>
<dl class="simple">
<dt>parameter_name<span class="classifier">str</span></dt><dd><p>Worker.algo attribute name</p>
</dd>
<dt>new_parameter_value<span class="classifier">int or float</span></dt><dd><p>New value for <cite>parameter_name</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nappo.core.algos.ppo">
<span id="nappo-core-algos-ppo-module"></span><h2>nappo.core.algos.ppo module<a class="headerlink" href="#module-nappo.core.algos.ppo" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nappo.core.algos.ppo.PPO">
<em class="property">class </em><code class="sig-prename descclassname">nappo.core.algos.ppo.</code><code class="sig-name descname">PPO</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device</span></em>, <em class="sig-param"><span class="n">actor_critic</span></em>, <em class="sig-param"><span class="n">lr</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-08</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">0.99</span></em>, <em class="sig-param"><span class="n">num_epochs</span><span class="o">=</span><span class="default_value">4</span></em>, <em class="sig-param"><span class="n">clip_param</span><span class="o">=</span><span class="default_value">0.2</span></em>, <em class="sig-param"><span class="n">num_mini_batch</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">test_every</span><span class="o">=</span><span class="default_value">20000</span></em>, <em class="sig-param"><span class="n">max_grad_norm</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">entropy_coef</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">value_loss_coef</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">num_test_episodes</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">use_clipped_value_loss</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nappo.core.algos.base.Algo" title="nappo.core.algos.base.Algo"><code class="xref py py-class docutils literal notranslate"><span class="pre">nappo.core.algos.base.Algo</span></code></a></p>
<p>Proximal Policy Optimization algorithm class.</p>
<p>Algorithm class to execute PPO, from Schulman et al.
(<a class="reference external" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a>). Algorithms are modules generally
required by multiple workers, so PPO.algo_factory(…) returns a function
that can be passed on to workers to instantiate their own PPO module.</p>
<dl class="simple">
<dt>device: torch.device</dt><dd><p>CPU or specific GPU where class computations will take place.</p>
</dd>
<dt>actor_critic<span class="classifier">ActorCritic</span></dt><dd><p>Actor_critic class instance.</p>
</dd>
<dt>lr<span class="classifier">float</span></dt><dd><p>Optimizer learning rate.</p>
</dd>
<dt>eps<span class="classifier">float</span></dt><dd><p>Optimizer epsilon parameter.</p>
</dd>
<dt>num_epochs<span class="classifier">int</span></dt><dd><p>Number of PPO epochs.</p>
</dd>
<dt>gamma<span class="classifier">float</span></dt><dd><p>Discount factor parameter.</p>
</dd>
<dt>clip_param<span class="classifier">float</span></dt><dd><p>PPO clipping parameter.</p>
</dd>
<dt>num_mini_batch<span class="classifier">int</span></dt><dd><p>Number of batches to create from collected data for actor_critic updates.</p>
</dd>
<dt>num_test_episodes<span class="classifier">int</span></dt><dd><p>Number of episodes to complete in each test phase.</p>
</dd>
<dt>test_every<span class="classifier">int</span></dt><dd><p>Regularity of test evaluations.</p>
</dd>
<dt>max_grad_norm<span class="classifier">float</span></dt><dd><p>Gradient clipping parameter.</p>
</dd>
<dt>entropy_coef<span class="classifier">float</span></dt><dd><p>PPO entropy coefficient parameter.</p>
</dd>
<dt>value_loss_coef<span class="classifier">float</span></dt><dd><p>PPO value coefficient parameter.</p>
</dd>
<dt>use_clipped_value_loss<span class="classifier">bool</span></dt><dd><p>Prevent value loss from shifting too fast.</p>
</dd>
</dl>
<dl class="simple">
<dt>start_steps<span class="classifier">int</span></dt><dd><p>Number of steps collected with initial random policy (default to 0 for
On-policy algos).</p>
</dd>
<dt>num_epochs<span class="classifier">int</span></dt><dd><p>Times data in the buffer is re-used before data collection proceeds.</p>
</dd>
<dt>update_every<span class="classifier">int</span></dt><dd><p>Number of data samples collected between network update stages (depends
on storage capacity for On-policy algos).</p>
</dd>
<dt>num_mini_batch<span class="classifier">int</span></dt><dd><p>Number mini batches per epoch.</p>
</dd>
<dt>mini_batch_size<span class="classifier">int</span></dt><dd><p>Size of update mini batches.</p>
</dd>
<dt>test_every<span class="classifier">int</span></dt><dd><p>Number of network updates between test evaluations.</p>
</dd>
<dt>num_test_episodes<span class="classifier">int</span></dt><dd><p>Num episodes to complete in each test phase.</p>
</dd>
<dt>device<span class="classifier">torch.device</span></dt><dd><p>CPU or specific GPU where class computations will take place.</p>
</dd>
<dt>actor_critic<span class="classifier">ActorCritic</span></dt><dd><p>ActorCritic Class containing Neural Network function approximators.</p>
</dd>
<dt>gamma<span class="classifier">float</span></dt><dd><p>Discount factor parameter.</p>
</dd>
<dt>clip_param<span class="classifier">float</span></dt><dd><p>PPO clipping parameter.</p>
</dd>
<dt>entropy_coef<span class="classifier">float</span></dt><dd><p>PPO entropy coefficient parameter.</p>
</dd>
<dt>max_grad_norm<span class="classifier">float</span></dt><dd><p>Gradient clipping parameter.</p>
</dd>
<dt>value_loss_coef<span class="classifier">float</span></dt><dd><p>PPO value coefficient parameter.</p>
</dd>
<dt>use_clipped_value_loss<span class="classifier">bool</span></dt><dd><p>Whether or not value loss clipping is being used</p>
</dd>
<dt>optimizer<span class="classifier">torch.optimizer</span></dt><dd><p>ActorCritic model optimizer</p>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ac</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">create_algo</span> <span class="o">=</span> <span class="n">PPO</span><span class="o">.</span><span class="n">algo_factory</span><span class="p">(</span>
<span class="go">    lr=0.01, eps=1e-5, num_epochs=4, clip_param=0.2,</span>
<span class="go">    entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5,</span>
<span class="go">    num_mini_batch=4, use_clipped_value_loss=True, gamma=0.99)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ppo1</span> <span class="o">=</span> <span class="n">create_algo</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ppo2</span> <span class="o">=</span> <span class="n">create_algo</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:2&quot;</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ac</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ppo</span> <span class="o">=</span> <span class="n">PPO</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">),</span> <span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="go">              eps=1e-5, num_epochs=4, clip_param=0.2, entropy_coef=0.01,</span>
<span class="go">              value_loss_coef=0.5, max_grad_norm=0.5, num_mini_batch=4,</span>
<span class="go">              use_clipped_value_loss=True, gamma=0.99)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.acting_step">
<code class="sig-name descname">acting_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obs</span></em>, <em class="sig-param"><span class="n">rhs</span></em>, <em class="sig-param"><span class="n">done</span></em>, <em class="sig-param"><span class="n">deterministic</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.acting_step" title="Permalink to this definition">¶</a></dt>
<dd><p>PPO acting function.</p>
<dl class="simple">
<dt>obs: torch.tensor</dt><dd><p>Current world observation</p>
</dd>
<dt>rhs: torch.tensor</dt><dd><p>RNN recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p>
</dd>
<dt>done: torch.tensor</dt><dd><p>1.0 if current obs is the last one in the episode, else 0.0.</p>
</dd>
<dt>deterministic: bool</dt><dd><p>Whether to randomly sample action from predicted distribution or take the mode.</p>
</dd>
</dl>
<dl class="simple">
<dt>action: torch.tensor</dt><dd><p>Predicted next action.</p>
</dd>
<dt>clipped_action: torch.tensor</dt><dd><p>Predicted next action (clipped to be within action space).</p>
</dd>
<dt>rhs: torch.tensor</dt><dd><p>Policy recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p>
</dd>
<dt>other: dict</dt><dd><p>Additional PPO predictions, value score and action log probability,
which are not used in other algorithms.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.algo_factory">
<em class="property">classmethod </em><code class="sig-name descname">algo_factory</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lr</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-08</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">0.99</span></em>, <em class="sig-param"><span class="n">num_epochs</span><span class="o">=</span><span class="default_value">4</span></em>, <em class="sig-param"><span class="n">clip_param</span><span class="o">=</span><span class="default_value">0.2</span></em>, <em class="sig-param"><span class="n">num_mini_batch</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">test_every</span><span class="o">=</span><span class="default_value">20000</span></em>, <em class="sig-param"><span class="n">max_grad_norm</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">entropy_coef</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">value_loss_coef</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">num_test_episodes</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">use_clipped_value_loss</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.algo_factory" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a function to create new PPO instances.</p>
<dl class="simple">
<dt>actor_critic<span class="classifier">ActorCritic</span></dt><dd><p>Actor_critic class instance.</p>
</dd>
<dt>lr<span class="classifier">float</span></dt><dd><p>Optimizer learning rate.</p>
</dd>
<dt>eps<span class="classifier">float</span></dt><dd><p>Optimizer epsilon parameter.</p>
</dd>
<dt>num_epochs<span class="classifier">int</span></dt><dd><p>Number of PPO epochs.</p>
</dd>
<dt>gamma<span class="classifier">float</span></dt><dd><p>Discount factor parameter.</p>
</dd>
<dt>clip_param<span class="classifier">float</span></dt><dd><p>PPO clipping parameter.</p>
</dd>
<dt>num_mini_batch<span class="classifier">int</span></dt><dd><p>Number of batches to create from collected data for actor_critic update.</p>
</dd>
<dt>num_test_episodes<span class="classifier">int</span></dt><dd><p>Number of episodes to complete in each test phase.</p>
</dd>
<dt>test_every<span class="classifier">int</span></dt><dd><p>Regularity of test evaluations.</p>
</dd>
<dt>max_grad_norm<span class="classifier">float</span></dt><dd><p>Gradient clipping parameter.</p>
</dd>
<dt>entropy_coef<span class="classifier">float</span></dt><dd><p>PPO entropy coefficient parameter.</p>
</dd>
<dt>value_loss_coef<span class="classifier">float</span></dt><dd><p>PPO value coefficient parameter.</p>
</dd>
<dt>use_clipped_value_loss<span class="classifier">bool</span></dt><dd><p>Prevent value loss from shifting too fast.</p>
</dd>
</dl>
<dl class="simple">
<dt>create_algo_instance<span class="classifier">func</span></dt><dd><p>creates a new PPO class instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.apply_gradients">
<code class="sig-name descname">apply_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gradients</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.apply_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Take an optimization step, previously setting new gradients if provided.</p>
<dl class="simple">
<dt>gradients: list of tensors</dt><dd><p>List of actor_critic gradients.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.compute_gradients">
<code class="sig-name descname">compute_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">grads_to_cpu</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.compute_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute loss and compute gradients but don’t do optimization step,
return gradients instead.</p>
<dl class="simple">
<dt>data: dict</dt><dd><p>data batch containing all required tensors to compute PPO loss.</p>
</dd>
<dt>grads_to_cpu: bool</dt><dd><p>If gradient tensor will be sent to another node, need to be in CPU.</p>
</dd>
</dl>
<dl class="simple">
<dt>grads: list of tensors</dt><dd><p>List of actor_critic gradients.</p>
</dd>
<dt>info: dict</dt><dd><p>Dict containing current PPO iteration information.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute PPO loss from data batch.</p>
<dl class="simple">
<dt>data: dict</dt><dd><p>Data batch dict containing all required tensors to compute PPO loss.</p>
</dd>
</dl>
<dl class="simple">
<dt>value_loss: torch.tensor</dt><dd><p>value term of PPO loss.</p>
</dd>
<dt>action_loss: torch.tensor</dt><dd><p>policy term of PPO loss.</p>
</dd>
<dt>dist_entropy: torch.tensor</dt><dd><p>policy term of PPO loss.</p>
</dd>
<dt>loss: torch.tensor</dt><dd><p>PPO loss.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.set_weights">
<code class="sig-name descname">set_weights</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.set_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Update actor critic with the given weights</p>
<dl class="simple">
<dt>weights: dict of tensors</dt><dd><p>Dict containing actor_critic weights to be set.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.update_algo_parameter">
<code class="sig-name descname">update_algo_parameter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parameter_name</span></em>, <em class="sig-param"><span class="n">new_parameter_value</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.update_algo_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>parameter_name</cite> is an attribute of the algorithm, change its value
to <cite>new_parameter_value value</cite>.</p>
<dl class="simple">
<dt>parameter_name<span class="classifier">str</span></dt><dd><p>Worker.algo attribute name</p>
</dd>
<dt>new_parameter_value<span class="classifier">int or float</span></dt><dd><p>New value for <cite>parameter_name</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nappo.core.algos.sac">
<span id="nappo-core-algos-sac-module"></span><h2>nappo.core.algos.sac module<a class="headerlink" href="#module-nappo.core.algos.sac" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nappo.core.algos.sac.SAC">
<em class="property">class </em><code class="sig-prename descclassname">nappo.core.algos.sac.</code><code class="sig-name descname">SAC</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device</span></em>, <em class="sig-param"><span class="n">actor_critic</span></em>, <em class="sig-param"><span class="n">lr</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-05</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">0.99</span></em>, <em class="sig-param"><span class="n">polyak</span><span class="o">=</span><span class="default_value">0.995</span></em>, <em class="sig-param"><span class="n">num_updates</span><span class="o">=</span><span class="default_value">50</span></em>, <em class="sig-param"><span class="n">update_every</span><span class="o">=</span><span class="default_value">50</span></em>, <em class="sig-param"><span class="n">test_every</span><span class="o">=</span><span class="default_value">5000</span></em>, <em class="sig-param"><span class="n">start_steps</span><span class="o">=</span><span class="default_value">20000</span></em>, <em class="sig-param"><span class="n">mini_batch_size</span><span class="o">=</span><span class="default_value">64</span></em>, <em class="sig-param"><span class="n">reward_scaling</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">num_test_episodes</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">target_update_interval</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nappo.core.algos.base.Algo" title="nappo.core.algos.base.Algo"><code class="xref py py-class docutils literal notranslate"><span class="pre">nappo.core.algos.base.Algo</span></code></a></p>
<p>Soft Actor Critic algorithm class.</p>
<p>Algorithm class to execute SAC, from Haarnoja et al.
(<a class="reference external" href="https://arxiv.org/abs/1812.05905">https://arxiv.org/abs/1812.05905</a>). Algorithms are modules generally
required by multiple workers, so SAC.algo_factory(…) returns a function
that can be passed on to workers to instantiate their own SAC module.</p>
<dl class="simple">
<dt>device<span class="classifier">torch.device</span></dt><dd><p>CPU or specific GPU where class computations will take place.</p>
</dd>
<dt>actor_critic<span class="classifier">ActorCritic</span></dt><dd><p>Actor_critic class instance.</p>
</dd>
<dt>lr<span class="classifier">float</span></dt><dd><p>Optimizers learning rate.</p>
</dd>
<dt>eps<span class="classifier">float</span></dt><dd><p>Optimizers epsilon parameter.</p>
</dd>
<dt>gamma<span class="classifier">float</span></dt><dd><p>Discount factor parameter.</p>
</dd>
<dt>alpha: float</dt><dd><p>Initial entropy coefficient value.</p>
</dd>
<dt>polyak: float</dt><dd><p>SAC polyak averaging parameter.</p>
</dd>
<dt>num_updates: int</dt><dd><p>Num consecutive actor_critic updates before data collection continues.</p>
</dd>
<dt>update_every: int</dt><dd><p>Regularity of actor_critic updates in number environment steps.</p>
</dd>
<dt>start_steps: int</dt><dd><p>Num of initial random environment steps before learning starts.</p>
</dd>
<dt>mini_batch_size: int</dt><dd><p>Size of actor_critic update batches.</p>
</dd>
<dt>reward_scaling: float</dt><dd><p>Reward scaling factor.</p>
</dd>
<dt>target_update_interval: float</dt><dd><p>regularity of target nets updates with respect to actor_critic Adam updates.</p>
</dd>
<dt>num_test_episodes<span class="classifier">int</span></dt><dd><p>Number of episodes to complete in each test phase.</p>
</dd>
<dt>test_every<span class="classifier">int</span></dt><dd><p>Regularity of test evaluations in actor_critic updates.</p>
</dd>
</dl>
<dl class="simple">
<dt>start_steps<span class="classifier">int</span></dt><dd><p>Number of steps collected with initial random policy (default to 0 for
On-policy algos).</p>
</dd>
<dt>num_epochs<span class="classifier">int</span></dt><dd><p>Times data in the buffer is re-used before data collection proceeds.</p>
</dd>
<dt>update_every<span class="classifier">int</span></dt><dd><p>Number of data samples collected between network update stages (depends
on storage capacity for On-policy algos).</p>
</dd>
<dt>num_mini_batch<span class="classifier">int</span></dt><dd><p>Number mini batches per epoch.</p>
</dd>
<dt>mini_batch_size<span class="classifier">int</span></dt><dd><p>Size of update mini batches.</p>
</dd>
<dt>test_every<span class="classifier">int</span></dt><dd><p>Number of network updates between test evaluations.</p>
</dd>
<dt>num_test_episodes<span class="classifier">int</span></dt><dd><p>Num episodes to complete in each test phase.</p>
</dd>
<dt>device<span class="classifier">torch.device</span></dt><dd><p>CPU or specific GPU where class computation take place.</p>
</dd>
<dt>actor_critic<span class="classifier">ActorCritic</span></dt><dd><p>ActorCritic Class containing Neural Network function approximators.</p>
</dd>
<dt>gamma<span class="classifier">float</span></dt><dd><p>Discount factor parameter.</p>
</dd>
<dt>iter<span class="classifier">int</span></dt><dd><p>Num actor_critic Adam updates.</p>
</dd>
<dt>polyak<span class="classifier">float</span></dt><dd><p>SAC polyak averaging parameter.</p>
</dd>
<dt>reward_scaling<span class="classifier">float</span></dt><dd><p>Reward scaling factor.</p>
</dd>
<dt>target_update_interval<span class="classifier">int</span></dt><dd><p>regularity of target nets updates with respect to actor_critic Adam updates.</p>
</dd>
<dt>log_alpha<span class="classifier">torch.tensor</span></dt><dd><p>Log entropy coefficient value.</p>
</dd>
<dt>alpha<span class="classifier">torch.tensor</span></dt><dd><p>Entropy coefficient value.</p>
</dd>
<dt>pi_optimizer<span class="classifier">torch.optimizer</span></dt><dd><p>Policy model optimizer.</p>
</dd>
<dt>q_optimizer<span class="classifier">torch.optimizer</span></dt><dd><p>Q critics model optimizer.</p>
</dd>
<dt>alpha_optimizer<span class="classifier">torch.optimizer</span></dt><dd><p>alpha parameter optimizer.</p>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ac</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">create_algo</span> <span class="o">=</span> <span class="n">SAC</span><span class="o">.</span><span class="n">algo_factory</span><span class="p">(</span>
<span class="go">        lr=1e-4, eps=1e-5, gamma=0.99, polyak=0.995, num_updates=50,</span>
<span class="go">        update_every=50, test_every=5000, start_steps=20000,</span>
<span class="go">        mini_batch_size=64, alpha=1.0, reward_scaling=1.0,</span>
<span class="go">        num_test_episodes=0, target_update_interval=1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sac1</span> <span class="o">=</span> <span class="n">create_algo</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sac2</span> <span class="o">=</span> <span class="n">create_algo</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:2&quot;</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ac</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sac</span> <span class="o">=</span> <span class="n">SAC</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">),</span> <span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
<span class="go">              eps=1e-5, gamma=0.99, polyak=0.995, num_updates=50,</span>
<span class="go">              update_every=50, test_every=5000, start_steps=20000,</span>
<span class="go">              mini_batch_size=64, alpha=1.0, reward_scaling=1.0,</span>
<span class="go">              num_test_episodes=0, target_update_interval=1)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.acting_step">
<code class="sig-name descname">acting_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obs</span></em>, <em class="sig-param"><span class="n">rhs</span></em>, <em class="sig-param"><span class="n">done</span></em>, <em class="sig-param"><span class="n">deterministic</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.acting_step" title="Permalink to this definition">¶</a></dt>
<dd><p>SAC acting function.</p>
<dl class="simple">
<dt>obs: torch.tensor</dt><dd><p>Current world observation</p>
</dd>
<dt>rhs: torch.tensor</dt><dd><p>RNN recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p>
</dd>
<dt>done: torch.tensor</dt><dd><p>1.0 if current obs is the last one in the episode, else 0.0.</p>
</dd>
<dt>deterministic: bool</dt><dd><p>Whether to randomly sample action from predicted distribution or taking the mode.</p>
</dd>
</dl>
<dl class="simple">
<dt>action: torch.tensor</dt><dd><p>Predicted next action.</p>
</dd>
<dt>clipped_action: torch.tensor</dt><dd><p>Predicted next action (clipped to be within action space).</p>
</dd>
<dt>rhs: torch.tensor</dt><dd><p>Policy recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p>
</dd>
<dt>other: dict</dt><dd><p>Additional SAC predictions, which are not used in other algorithms.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.algo_factory">
<em class="property">classmethod </em><code class="sig-name descname">algo_factory</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lr</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-05</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">0.99</span></em>, <em class="sig-param"><span class="n">polyak</span><span class="o">=</span><span class="default_value">0.995</span></em>, <em class="sig-param"><span class="n">num_updates</span><span class="o">=</span><span class="default_value">50</span></em>, <em class="sig-param"><span class="n">update_every</span><span class="o">=</span><span class="default_value">50</span></em>, <em class="sig-param"><span class="n">start_steps</span><span class="o">=</span><span class="default_value">20000</span></em>, <em class="sig-param"><span class="n">mini_batch_size</span><span class="o">=</span><span class="default_value">64</span></em>, <em class="sig-param"><span class="n">reward_scaling</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">target_update_interval</span><span class="o">=</span><span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.algo_factory" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a function to create new SAC instances.</p>
<dl class="simple">
<dt>lr<span class="classifier">float</span></dt><dd><p>Optimizers learning rate.</p>
</dd>
<dt>eps<span class="classifier">float</span></dt><dd><p>Optimizers epsilon parameter.</p>
</dd>
<dt>gamma<span class="classifier">float</span></dt><dd><p>Discount factor parameter.</p>
</dd>
<dt>alpha: float</dt><dd><p>Initial entropy coefficient value.</p>
</dd>
<dt>polyak: float</dt><dd><p>SAC polyak averaging parameter.</p>
</dd>
<dt>num_updates: int</dt><dd><p>Num consecutive actor_critic updates before data collection continues.</p>
</dd>
<dt>update_every: int</dt><dd><p>Regularity of actor_critic updates in number environment steps.</p>
</dd>
<dt>start_steps: int</dt><dd><p>Num of initial random environment steps before learning starts.</p>
</dd>
<dt>mini_batch_size: int</dt><dd><p>Size of actor_critic update batches.</p>
</dd>
<dt>reward_scaling: float</dt><dd><p>Reward scaling factor.</p>
</dd>
<dt>target_update_interval: float</dt><dd><p>regularity of target nets updates with respect to actor_critic Adam updates.</p>
</dd>
<dt>num_test_episodes<span class="classifier">int</span></dt><dd><p>Number of episodes to complete in each test phase.</p>
</dd>
<dt>test_every<span class="classifier">int</span></dt><dd><p>Regularity of test evaluations in actor_critic updates.</p>
</dd>
</dl>
<dl class="simple">
<dt>create_algo_instance<span class="classifier">func</span></dt><dd><p>creates a new SAC class instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.apply_gradients">
<code class="sig-name descname">apply_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gradients</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.apply_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Take an optimization step, previously setting new gradients if provided.
Update also target networks.</p>
<dl class="simple">
<dt>gradients: list of tensors</dt><dd><p>List of actor_critic gradients.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.calculate_target_entropy">
<code class="sig-name descname">calculate_target_entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.calculate_target_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate SAC target entropy</p>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.compute_gradients">
<code class="sig-name descname">compute_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">grads_to_cpu</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.compute_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute loss and compute gradients but don’t do optimization step,
return gradients instead.</p>
<dl class="simple">
<dt>data: dict</dt><dd><p>data batch containing all required tensors to compute SAC losses.</p>
</dd>
<dt>grads_to_cpu: bool</dt><dd><p>If gradient tensor will be sent to another node, need to be in CPU.</p>
</dd>
</dl>
<dl class="simple">
<dt>grads: list of tensors</dt><dd><p>List of actor_critic gradients.</p>
</dd>
<dt>info: dict</dt><dd><p>Dict containing current SAC iteration information.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.compute_loss_alpha">
<code class="sig-name descname">compute_loss_alpha</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_probs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.compute_loss_alpha" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate SAC entropy loss.</p>
<p>log_probs: torch.tensor</p>
<dl class="simple">
<dt>alpha_loss: torch.tensor</dt><dd><p>SAC entropy loss.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.compute_loss_pi">
<code class="sig-name descname">compute_loss_pi</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.compute_loss_pi" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate SAC policy loss.</p>
<dl class="simple">
<dt>data: dict</dt><dd><p>Data batch dict containing all required tensors to compute SAC losses.</p>
</dd>
</dl>
<dl class="simple">
<dt>loss_pi<span class="classifier">torch.tensor</span></dt><dd><p>SAC policy loss.</p>
</dd>
<dt>logp_pi<span class="classifier">torch.tensor</span></dt><dd><p>Log probability of predicted next action.</p>
</dd>
<dt>rnn_hs<span class="classifier">torch.tensor</span></dt><dd><p>Policy recurrent hidden state obtained with the current ActorCritic version.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.compute_loss_q">
<code class="sig-name descname">compute_loss_q</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="n">rnn_hs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.compute_loss_q" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate SAC Q-nets loss</p>
<dl class="simple">
<dt>data: dict</dt><dd><p>Data batch dict containing all required tensors to compute SAC losses.</p>
</dd>
<dt>rnn_hs<span class="classifier">torch.tensor</span></dt><dd><p>Policy recurrent hidden state obtained with the current ActorCritic version.</p>
</dd>
</dl>
<dl class="simple">
<dt>loss_q1<span class="classifier">torch.tensor</span></dt><dd><p>Q1-net loss.</p>
</dd>
<dt>loss_q2<span class="classifier">torch.tensor</span></dt><dd><p>Q2-net loss.</p>
</dd>
<dt>loss_q<span class="classifier">torch.tensor</span></dt><dd><p>Weighted average of loss_q1 and loss_q2.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.discrete_version">
<em class="property">property </em><code class="sig-name descname">discrete_version</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.discrete_version" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if action_space is discrete.</p>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.set_weights">
<code class="sig-name descname">set_weights</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.set_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Update actor critic with the given weights. Update also target networks.</p>
<dl class="simple">
<dt>weights: dict of tensors</dt><dd><p>Dict containing actor_critic weights to be set.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.update_algo_parameter">
<code class="sig-name descname">update_algo_parameter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parameter_name</span></em>, <em class="sig-param"><span class="n">new_parameter_value</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.update_algo_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>parameter_name</cite> is an attribute of the algorithm, change its value
to <cite>new_parameter_value value</cite>.</p>
<dl class="simple">
<dt>parameter_name<span class="classifier">str</span></dt><dd><p>Worker.algo attribute name</p>
</dd>
<dt>new_parameter_value<span class="classifier">int or float</span></dt><dd><p>New value for <cite>parameter_name</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.update_target_networks">
<code class="sig-name descname">update_target_networks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.update_target_networks" title="Permalink to this definition">¶</a></dt>
<dd><p>Update actor critic target networks with polyak averaging</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nappo.core.algos">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-nappo.core.algos" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">nappo.core.algos package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-nappo.core.algos.base">nappo.core.algos.base module</a></li>
<li><a class="reference internal" href="#module-nappo.core.algos.ppo">nappo.core.algos.ppo module</a></li>
<li><a class="reference internal" href="#module-nappo.core.algos.sac">nappo.core.algos.sac module</a></li>
<li><a class="reference internal" href="#module-nappo.core.algos">Module contents</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="nappo.core.html"
                        title="previous chapter">nappo.core package</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="nappo.core.envs.html"
                        title="next chapter">nappo.core.envs package</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/nappo.core.algos.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="nappo.core.envs.html" title="nappo.core.envs package"
             >next</a> |</li>
        <li class="right" >
          <a href="nappo.core.html" title="nappo.core package"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">nappo 0.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="nappo.html" >nappo package</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="nappo.core.html" >nappo.core package</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, nappo.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.3.
    </div>
  </body>
</html>

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>nappo.core.algos package &#8212; nappo 0.0.22 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="nappo.core.envs package" href="nappo.core.envs.html" />
    <link rel="prev" title="nappo.core.actors.neural_networks.feature_extractors package" href="nappo.core.actors.neural_networks.feature_extractors.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="nappo.core.envs.html" title="nappo.core.envs package"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="nappo.core.actors.neural_networks.feature_extractors.html" title="nappo.core.actors.neural_networks.feature_extractors package"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">nappo 0.0.22 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="nappo.html" >nappo package</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="nappo.core.html" accesskey="U">nappo.core package</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="nappo-core-algos-package">
<h1>nappo.core.algos package<a class="headerlink" href="#nappo-core-algos-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-nappo.core.algos.base">
<span id="nappo-core-algos-base-module"></span><h2>nappo.core.algos.base module<a class="headerlink" href="#module-nappo.core.algos.base" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nappo.core.algos.base.Algo">
<em class="property">class </em><code class="sig-prename descclassname">nappo.core.algos.base.</code><code class="sig-name descname">Algo</code><a class="headerlink" href="#nappo.core.algos.base.Algo" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Base class for all algorithms</p>
<dl class="py method">
<dt id="nappo.core.algos.base.Algo.acting_step">
<em class="property">abstract </em><code class="sig-name descname">acting_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obs</span></em>, <em class="sig-param"><span class="n">rhs</span></em>, <em class="sig-param"><span class="n">done</span></em>, <em class="sig-param"><span class="n">deterministic</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.acting_step" title="Permalink to this definition">¶</a></dt>
<dd><p>PPO acting function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>obs</strong> (<em>torch.tensor</em>) – Current world observation</p></li>
<li><p><strong>rhs</strong> (<em>torch.tensor</em>) – RNN recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p></li>
<li><p><strong>done</strong> (<em>torch.tensor</em>) – 1.0 if current obs is the last one in the episode, else 0.0.</p></li>
<li><p><strong>deterministic</strong> (<em>bool</em>) – Whether to randomly sample action from predicted distribution or take the mode.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>action</strong> (<em>torch.tensor</em>) – Predicted next action.</p></li>
<li><p><strong>clipped_action</strong> (<em>torch.tensor</em>) – Predicted next action (clipped to be within action space).</p></li>
<li><p><strong>rhs</strong> (<em>torch.tensor</em>) – Policy recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p></li>
<li><p><strong>other</strong> (<em>dict</em>) – Additional PPO predictions, value score and action log probability,
which are not used in other algorithms.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.base.Algo.apply_gradients">
<em class="property">abstract </em><code class="sig-name descname">apply_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gradients</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.apply_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Take an optimization step, previously setting new gradients if provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<em>list of tensors</em>) – List of actor_critic gradients.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.base.Algo.compute_gradients">
<em class="property">abstract </em><code class="sig-name descname">compute_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">grads_to_cpu</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.compute_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute loss and compute gradients but don’t do optimization step,
return gradients instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>dict</em>) – data batch containing all required tensors to compute PPO loss.</p></li>
<li><p><strong>grads_to_cpu</strong> (<em>bool</em>) – If gradient tensor will be sent to another node, need to be in CPU.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>grads</strong> (<em>list of tensors</em>) – List of actor_critic gradients.</p></li>
<li><p><strong>info</strong> (<em>dict</em>) – Dict containing current PPO iteration information.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.base.Algo.create_factory">
<em class="property">abstract classmethod </em><code class="sig-name descname">create_factory</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.create_factory" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a function to create new Algo instances</p>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.base.Algo.set_weights">
<em class="property">abstract </em><code class="sig-name descname">set_weights</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.set_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Update actor critic with the given weights</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weights</strong> (<em>dict of tensors</em>) – Dict containing actor_critic weights to be set.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.base.Algo.update_algo_parameter">
<em class="property">abstract </em><code class="sig-name descname">update_algo_parameter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parameter_name</span></em>, <em class="sig-param"><span class="n">new_parameter_value</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.base.Algo.update_algo_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>parameter_name</cite> is an attribute of the algorithm, change its value
to <cite>new_parameter_value value</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameter_name</strong> (<em>str</em>) – Worker.algo attribute name</p></li>
<li><p><strong>new_parameter_value</strong> (<em>int</em><em> or </em><em>float</em>) – New value for <cite>parameter_name</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nappo.core.algos.ppo">
<span id="nappo-core-algos-ppo-module"></span><h2>nappo.core.algos.ppo module<a class="headerlink" href="#module-nappo.core.algos.ppo" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nappo.core.algos.ppo.PPO">
<em class="property">class </em><code class="sig-prename descclassname">nappo.core.algos.ppo.</code><code class="sig-name descname">PPO</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device</span></em>, <em class="sig-param"><span class="n">actor_critic</span></em>, <em class="sig-param"><span class="n">lr</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-08</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">0.99</span></em>, <em class="sig-param"><span class="n">num_epochs</span><span class="o">=</span><span class="default_value">4</span></em>, <em class="sig-param"><span class="n">clip_param</span><span class="o">=</span><span class="default_value">0.2</span></em>, <em class="sig-param"><span class="n">num_mini_batch</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">test_every</span><span class="o">=</span><span class="default_value">20000</span></em>, <em class="sig-param"><span class="n">max_grad_norm</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">entropy_coef</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">value_loss_coef</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">num_test_episodes</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">use_clipped_value_loss</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nappo.core.algos.base.Algo" title="nappo.core.algos.base.Algo"><code class="xref py py-class docutils literal notranslate"><span class="pre">nappo.core.algos.base.Algo</span></code></a></p>
<p>Proximal Policy Optimization algorithm class.</p>
<p>Algorithm class to execute PPO, from Schulman et al.
(<a class="reference external" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a>). Algorithms are modules generally
required by multiple workers, so PPO.algo_factory(…) returns a function
that can be passed on to workers to instantiate their own PPO module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<em>torch.device</em>) – CPU or specific GPU where class computations will take place.</p></li>
<li><p><strong>actor_critic</strong> (<em>ActorCritic</em>) – Actor_critic class instance.</p></li>
<li><p><strong>lr</strong> (<em>float</em>) – Optimizer learning rate.</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – Optimizer epsilon parameter.</p></li>
<li><p><strong>num_epochs</strong> (<em>int</em>) – Number of PPO epochs.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – Discount factor parameter.</p></li>
<li><p><strong>clip_param</strong> (<em>float</em>) – PPO clipping parameter.</p></li>
<li><p><strong>num_mini_batch</strong> (<em>int</em>) – Number of batches to create from collected data for actor_critic updates.</p></li>
<li><p><strong>num_test_episodes</strong> (<em>int</em>) – Number of episodes to complete in each test phase.</p></li>
<li><p><strong>test_every</strong> (<em>int</em>) – Regularity of test evaluations.</p></li>
<li><p><strong>max_grad_norm</strong> (<em>float</em>) – Gradient clipping parameter.</p></li>
<li><p><strong>entropy_coef</strong> (<em>float</em>) – PPO entropy coefficient parameter.</p></li>
<li><p><strong>value_loss_coef</strong> (<em>float</em>) – PPO value coefficient parameter.</p></li>
<li><p><strong>use_clipped_value_loss</strong> (<em>bool</em>) – Prevent value loss from shifting too fast.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.start_steps">
<code class="sig-name descname">start_steps</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.start_steps" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of steps collected with initial random policy (default to 0 for
On-policy algos).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.num_epochs">
<code class="sig-name descname">num_epochs</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.num_epochs" title="Permalink to this definition">¶</a></dt>
<dd><p>Times data in the buffer is re-used before data collection proceeds.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.update_every">
<code class="sig-name descname">update_every</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.update_every" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of data samples collected between network update stages (depends
on storage capacity for On-policy algos).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.num_mini_batch">
<code class="sig-name descname">num_mini_batch</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.num_mini_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Number mini batches per epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.mini_batch_size">
<code class="sig-name descname">mini_batch_size</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.mini_batch_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Size of update mini batches.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.test_every">
<code class="sig-name descname">test_every</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.test_every" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of network updates between test evaluations.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.num_test_episodes">
<code class="sig-name descname">num_test_episodes</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.num_test_episodes" title="Permalink to this definition">¶</a></dt>
<dd><p>Num episodes to complete in each test phase.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.device">
<code class="sig-name descname">device</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.device" title="Permalink to this definition">¶</a></dt>
<dd><p>CPU or specific GPU where class computations will take place.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.device</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.actor_critic">
<code class="sig-name descname">actor_critic</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.actor_critic" title="Permalink to this definition">¶</a></dt>
<dd><p>ActorCritic Class containing Neural Network function approximators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ActorCritic</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.gamma">
<code class="sig-name descname">gamma</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.gamma" title="Permalink to this definition">¶</a></dt>
<dd><p>Discount factor parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.clip_param">
<code class="sig-name descname">clip_param</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.clip_param" title="Permalink to this definition">¶</a></dt>
<dd><p>PPO clipping parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.entropy_coef">
<code class="sig-name descname">entropy_coef</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.entropy_coef" title="Permalink to this definition">¶</a></dt>
<dd><p>PPO entropy coefficient parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.max_grad_norm">
<code class="sig-name descname">max_grad_norm</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.max_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient clipping parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.value_loss_coef">
<code class="sig-name descname">value_loss_coef</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.value_loss_coef" title="Permalink to this definition">¶</a></dt>
<dd><p>PPO value coefficient parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.use_clipped_value_loss">
<code class="sig-name descname">use_clipped_value_loss</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.use_clipped_value_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether or not value loss clipping is being used</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.ppo.PPO.optimizer">
<code class="sig-name descname">optimizer</code><a class="headerlink" href="#nappo.core.algos.ppo.PPO.optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>ActorCritic model optimizer</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.optimizer</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ac</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">create_algo</span> <span class="o">=</span> <span class="n">PPO</span><span class="o">.</span><span class="n">algo_factory</span><span class="p">(</span>
<span class="go">    lr=0.01, eps=1e-5, num_epochs=4, clip_param=0.2,</span>
<span class="go">    entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5,</span>
<span class="go">    num_mini_batch=4, use_clipped_value_loss=True, gamma=0.99)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ppo1</span> <span class="o">=</span> <span class="n">create_algo</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ppo2</span> <span class="o">=</span> <span class="n">create_algo</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:2&quot;</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ac</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ppo</span> <span class="o">=</span> <span class="n">PPO</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">),</span> <span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="go">              eps=1e-5, num_epochs=4, clip_param=0.2, entropy_coef=0.01,</span>
<span class="go">              value_loss_coef=0.5, max_grad_norm=0.5, num_mini_batch=4,</span>
<span class="go">              use_clipped_value_loss=True, gamma=0.99)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.acting_step">
<code class="sig-name descname">acting_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obs</span></em>, <em class="sig-param"><span class="n">rhs</span></em>, <em class="sig-param"><span class="n">done</span></em>, <em class="sig-param"><span class="n">deterministic</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.acting_step" title="Permalink to this definition">¶</a></dt>
<dd><p>PPO acting function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>obs</strong> (<em>torch.tensor</em>) – Current world observation</p></li>
<li><p><strong>rhs</strong> (<em>torch.tensor</em>) – RNN recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p></li>
<li><p><strong>done</strong> (<em>torch.tensor</em>) – 1.0 if current obs is the last one in the episode, else 0.0.</p></li>
<li><p><strong>deterministic</strong> (<em>bool</em>) – Whether to randomly sample action from predicted distribution or take the mode.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>action</strong> (<em>torch.tensor</em>) – Predicted next action.</p></li>
<li><p><strong>clipped_action</strong> (<em>torch.tensor</em>) – Predicted next action (clipped to be within action space).</p></li>
<li><p><strong>rhs</strong> (<em>torch.tensor</em>) – Policy recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p></li>
<li><p><strong>other</strong> (<em>dict</em>) – Additional PPO predictions, value score and action log probability,
which are not used in other algorithms.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.apply_gradients">
<code class="sig-name descname">apply_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gradients</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.apply_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Take an optimization step, previously setting new gradients if provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<em>list of tensors</em>) – List of actor_critic gradients.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.compute_gradients">
<code class="sig-name descname">compute_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">grads_to_cpu</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.compute_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute loss and compute gradients but don’t do optimization step,
return gradients instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>dict</em>) – data batch containing all required tensors to compute PPO loss.</p></li>
<li><p><strong>grads_to_cpu</strong> (<em>bool</em>) – If gradient tensor will be sent to another node, need to be in CPU.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>grads</strong> (<em>list of tensors</em>) – List of actor_critic gradients.</p></li>
<li><p><strong>info</strong> (<em>dict</em>) – Dict containing current PPO iteration information.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute PPO loss from data batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>data</strong> (<em>dict</em>) – Data batch dict containing all required tensors to compute PPO loss.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>value_loss</strong> (<em>torch.tensor</em>) – value term of PPO loss.</p></li>
<li><p><strong>action_loss</strong> (<em>torch.tensor</em>) – policy term of PPO loss.</p></li>
<li><p><strong>dist_entropy</strong> (<em>torch.tensor</em>) – policy term of PPO loss.</p></li>
<li><p><strong>loss</strong> (<em>torch.tensor</em>) – PPO loss.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.create_factory">
<em class="property">classmethod </em><code class="sig-name descname">create_factory</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lr</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-08</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">0.99</span></em>, <em class="sig-param"><span class="n">num_epochs</span><span class="o">=</span><span class="default_value">4</span></em>, <em class="sig-param"><span class="n">clip_param</span><span class="o">=</span><span class="default_value">0.2</span></em>, <em class="sig-param"><span class="n">num_mini_batch</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">test_every</span><span class="o">=</span><span class="default_value">20000</span></em>, <em class="sig-param"><span class="n">max_grad_norm</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">entropy_coef</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">value_loss_coef</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">num_test_episodes</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">use_clipped_value_loss</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.create_factory" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a function to create new PPO instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor_critic</strong> (<em>ActorCritic</em>) – Actor_critic class instance.</p></li>
<li><p><strong>lr</strong> (<em>float</em>) – Optimizer learning rate.</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – Optimizer epsilon parameter.</p></li>
<li><p><strong>num_epochs</strong> (<em>int</em>) – Number of PPO epochs.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – Discount factor parameter.</p></li>
<li><p><strong>clip_param</strong> (<em>float</em>) – PPO clipping parameter.</p></li>
<li><p><strong>num_mini_batch</strong> (<em>int</em>) – Number of batches to create from collected data for actor_critic update.</p></li>
<li><p><strong>num_test_episodes</strong> (<em>int</em>) – Number of episodes to complete in each test phase.</p></li>
<li><p><strong>test_every</strong> (<em>int</em>) – Regularity of test evaluations.</p></li>
<li><p><strong>max_grad_norm</strong> (<em>float</em>) – Gradient clipping parameter.</p></li>
<li><p><strong>entropy_coef</strong> (<em>float</em>) – PPO entropy coefficient parameter.</p></li>
<li><p><strong>value_loss_coef</strong> (<em>float</em>) – PPO value coefficient parameter.</p></li>
<li><p><strong>use_clipped_value_loss</strong> (<em>bool</em>) – Prevent value loss from shifting too fast.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>create_algo_instance</strong> – creates a new PPO class instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>func</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.set_weights">
<code class="sig-name descname">set_weights</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.set_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Update actor critic with the given weights</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weights</strong> (<em>dict of tensors</em>) – Dict containing actor_critic weights to be set.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.ppo.PPO.update_algo_parameter">
<code class="sig-name descname">update_algo_parameter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parameter_name</span></em>, <em class="sig-param"><span class="n">new_parameter_value</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.ppo.PPO.update_algo_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>parameter_name</cite> is an attribute of the algorithm, change its value
to <cite>new_parameter_value value</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameter_name</strong> (<em>str</em>) – Worker.algo attribute name</p></li>
<li><p><strong>new_parameter_value</strong> (<em>int</em><em> or </em><em>float</em>) – New value for <cite>parameter_name</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nappo.core.algos.sac">
<span id="nappo-core-algos-sac-module"></span><h2>nappo.core.algos.sac module<a class="headerlink" href="#module-nappo.core.algos.sac" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nappo.core.algos.sac.SAC">
<em class="property">class </em><code class="sig-prename descclassname">nappo.core.algos.sac.</code><code class="sig-name descname">SAC</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device</span></em>, <em class="sig-param"><span class="n">actor_critic</span></em>, <em class="sig-param"><span class="n">lr_q</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">lr_pi</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">lr_alpha</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">0.99</span></em>, <em class="sig-param"><span class="n">polyak</span><span class="o">=</span><span class="default_value">0.995</span></em>, <em class="sig-param"><span class="n">num_updates</span><span class="o">=</span><span class="default_value">50</span></em>, <em class="sig-param"><span class="n">update_every</span><span class="o">=</span><span class="default_value">50</span></em>, <em class="sig-param"><span class="n">test_every</span><span class="o">=</span><span class="default_value">5000</span></em>, <em class="sig-param"><span class="n">initial_alpha</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">start_steps</span><span class="o">=</span><span class="default_value">20000</span></em>, <em class="sig-param"><span class="n">mini_batch_size</span><span class="o">=</span><span class="default_value">64</span></em>, <em class="sig-param"><span class="n">reward_scaling</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">num_test_episodes</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">target_update_interval</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nappo.core.algos.base.Algo" title="nappo.core.algos.base.Algo"><code class="xref py py-class docutils literal notranslate"><span class="pre">nappo.core.algos.base.Algo</span></code></a></p>
<p>Soft Actor Critic algorithm class.</p>
<p>Algorithm class to execute SAC, from Haarnoja et al.
(<a class="reference external" href="https://arxiv.org/abs/1812.05905">https://arxiv.org/abs/1812.05905</a>). Algorithms are modules generally
required by multiple workers, so SAC.algo_factory(…) returns a function
that can be passed on to workers to instantiate their own SAC module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<em>torch.device</em>) – CPU or specific GPU where class computations will take place.</p></li>
<li><p><strong>actor_critic</strong> (<em>ActorCritic</em>) – Actor_critic class instance.</p></li>
<li><p><strong>lr_pi</strong> (<em>float</em>) – Policy optimizer learning rate.</p></li>
<li><p><strong>lr_q</strong> (<em>float</em>) – Q-nets optimizer learning rate.</p></li>
<li><p><strong>lr_alpha</strong> (<em>float</em>) – Alpha optimizer learning rate.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – Discount factor parameter.</p></li>
<li><p><strong>initial_alpha</strong> (<em>float</em>) – Initial entropy coefficient value (temperature).</p></li>
<li><p><strong>polyak</strong> (<em>float</em>) – SAC polyak averaging parameter.</p></li>
<li><p><strong>num_updates</strong> (<em>int</em>) – Num consecutive actor_critic updates before data collection continues.</p></li>
<li><p><strong>update_every</strong> (<em>int</em>) – Regularity of actor_critic updates in number environment steps.</p></li>
<li><p><strong>start_steps</strong> (<em>int</em>) – Num of initial random environment steps before learning starts.</p></li>
<li><p><strong>mini_batch_size</strong> (<em>int</em>) – Size of actor_critic update batches.</p></li>
<li><p><strong>reward_scaling</strong> (<em>float</em>) – Reward scaling factor.</p></li>
<li><p><strong>target_update_interval</strong> (<em>float</em>) – regularity of target nets updates with respect to actor_critic Adam updates.</p></li>
<li><p><strong>num_test_episodes</strong> (<em>int</em>) – Number of episodes to complete in each test phase.</p></li>
<li><p><strong>test_every</strong> (<em>int</em>) – Regularity of test evaluations in actor_critic updates.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.start_steps">
<code class="sig-name descname">start_steps</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.start_steps" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of steps collected with initial random policy (default to 0 for
On-policy algos).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.num_epochs">
<code class="sig-name descname">num_epochs</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.num_epochs" title="Permalink to this definition">¶</a></dt>
<dd><p>Times data in the buffer is re-used before data collection proceeds.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.update_every">
<code class="sig-name descname">update_every</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.update_every" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of data samples collected between network update stages (depends
on storage capacity for On-policy algos).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.num_mini_batch">
<code class="sig-name descname">num_mini_batch</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.num_mini_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Number mini batches per epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.mini_batch_size">
<code class="sig-name descname">mini_batch_size</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.mini_batch_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Size of update mini batches.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.test_every">
<code class="sig-name descname">test_every</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.test_every" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of network updates between test evaluations.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.num_test_episodes">
<code class="sig-name descname">num_test_episodes</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.num_test_episodes" title="Permalink to this definition">¶</a></dt>
<dd><p>Num episodes to complete in each test phase.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.device">
<code class="sig-name descname">device</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.device" title="Permalink to this definition">¶</a></dt>
<dd><p>CPU or specific GPU where class computation take place.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.device</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.actor_critic">
<code class="sig-name descname">actor_critic</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.actor_critic" title="Permalink to this definition">¶</a></dt>
<dd><p>ActorCritic Class containing Neural Network function approximators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ActorCritic</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.gamma">
<code class="sig-name descname">gamma</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.gamma" title="Permalink to this definition">¶</a></dt>
<dd><p>Discount factor parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.iter">
<code class="sig-name descname">iter</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.iter" title="Permalink to this definition">¶</a></dt>
<dd><p>Num actor_critic Adam updates.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.polyak">
<code class="sig-name descname">polyak</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.polyak" title="Permalink to this definition">¶</a></dt>
<dd><p>SAC polyak averaging parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.reward_scaling">
<code class="sig-name descname">reward_scaling</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.reward_scaling" title="Permalink to this definition">¶</a></dt>
<dd><p>Reward scaling factor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.target_update_interval">
<code class="sig-name descname">target_update_interval</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.target_update_interval" title="Permalink to this definition">¶</a></dt>
<dd><p>regularity of target nets updates with respect to actor_critic Adam updates.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.log_alpha">
<code class="sig-name descname">log_alpha</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.log_alpha" title="Permalink to this definition">¶</a></dt>
<dd><p>Log entropy coefficient value.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.alpha">
<code class="sig-name descname">alpha</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.alpha" title="Permalink to this definition">¶</a></dt>
<dd><p>Entropy coefficient value.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.pi_optimizer">
<code class="sig-name descname">pi_optimizer</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.pi_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Policy model optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.optimizer</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.q_optimizer">
<code class="sig-name descname">q_optimizer</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.q_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Q critics model optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.optimizer</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="nappo.core.algos.sac.SAC.alpha_optimizer">
<code class="sig-name descname">alpha_optimizer</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.alpha_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>alpha parameter optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.optimizer</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ac</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">create_algo</span> <span class="o">=</span> <span class="n">SAC</span><span class="o">.</span><span class="n">algo_factory</span><span class="p">(</span>
<span class="go">        lr_q=1e-4, lr_pi=1e-4, lr_alpha=1e-4, gamma=0.99, polyak=0.995,</span>
<span class="go">        num_updates=50, update_every=50, test_every=5000, start_steps=20000,</span>
<span class="go">        mini_batch_size=64, alpha=1.0, reward_scaling=1.0, num_test_episodes=0,</span>
<span class="go">         target_update_interval=1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sac1</span> <span class="o">=</span> <span class="n">create_algo</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sac2</span> <span class="o">=</span> <span class="n">create_algo</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:2&quot;</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ac</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sac</span> <span class="o">=</span> <span class="n">SAC</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">),</span> <span class="n">actor_critic</span><span class="o">=</span><span class="n">ac</span><span class="p">,</span> <span class="n">lr_q</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
<span class="go">              lr_pi=1e-4, lr_alpha=1e-4 gamma=0.99, polyak=0.995,</span>
<span class="go">              num_updates=50, update_every=50, test_every=5000,</span>
<span class="go">              start_steps=20000, mini_batch_size=64, alpha=1.0,</span>
<span class="go">              reward_scaling=1.0, num_test_episodes=0,</span>
<span class="go">              target_update_interval=1)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.acting_step">
<code class="sig-name descname">acting_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obs</span></em>, <em class="sig-param"><span class="n">rhs</span></em>, <em class="sig-param"><span class="n">done</span></em>, <em class="sig-param"><span class="n">deterministic</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.acting_step" title="Permalink to this definition">¶</a></dt>
<dd><p>SAC acting function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>obs</strong> (<em>torch.tensor</em>) – Current world observation</p></li>
<li><p><strong>rhs</strong> (<em>torch.tensor</em>) – RNN recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p></li>
<li><p><strong>done</strong> (<em>torch.tensor</em>) – 1.0 if current obs is the last one in the episode, else 0.0.</p></li>
<li><p><strong>deterministic</strong> (<em>bool</em>) – Whether to randomly sample action from predicted distribution or taking the mode.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>action</strong> (<em>torch.tensor</em>) – Predicted next action.</p></li>
<li><p><strong>clipped_action</strong> (<em>torch.tensor</em>) – Predicted next action (clipped to be within action space).</p></li>
<li><p><strong>rhs</strong> (<em>torch.tensor</em>) – Policy recurrent hidden state (if policy is not a RNN, rhs will contain zeroes).</p></li>
<li><p><strong>other</strong> (<em>dict</em>) – Additional SAC predictions, which are not used in other algorithms.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.apply_gradients">
<code class="sig-name descname">apply_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gradients</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.apply_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Take an optimization step, previously setting new gradients if provided.
Update also target networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<em>list of tensors</em>) – List of actor_critic gradients.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.calculate_target_entropy">
<code class="sig-name descname">calculate_target_entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.calculate_target_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate SAC target entropy</p>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.compute_gradients">
<code class="sig-name descname">compute_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">grads_to_cpu</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.compute_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute loss and compute gradients but don’t do optimization step,
return gradients instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>dict</em>) – data batch containing all required tensors to compute SAC losses.</p></li>
<li><p><strong>grads_to_cpu</strong> (<em>bool</em>) – If gradient tensor will be sent to another node, need to be in CPU.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>grads</strong> (<em>list of tensors</em>) – List of actor_critic gradients.</p></li>
<li><p><strong>info</strong> (<em>dict</em>) – Dict containing current SAC iteration information.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.compute_loss_alpha">
<code class="sig-name descname">compute_loss_alpha</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_probs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.compute_loss_alpha" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate SAC entropy loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>log_probs</strong> (<em>torch.tensor</em>) – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>alpha_loss</strong> – SAC entropy loss.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.compute_loss_pi">
<code class="sig-name descname">compute_loss_pi</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.compute_loss_pi" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate SAC policy loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>data</strong> (<em>dict</em>) – Data batch dict containing all required tensors to compute SAC losses.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>loss_pi</strong> (<em>torch.tensor</em>) – SAC policy loss.</p></li>
<li><p><strong>logp_pi</strong> (<em>torch.tensor</em>) – Log probability of predicted next action.</p></li>
<li><p><strong>rnn_hs</strong> (<em>torch.tensor</em>) – Policy recurrent hidden state obtained with the current ActorCritic version.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.compute_loss_q">
<code class="sig-name descname">compute_loss_q</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="n">rnn_hs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.compute_loss_q" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate SAC Q-nets loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>dict</em>) – Data batch dict containing all required tensors to compute SAC losses.</p></li>
<li><p><strong>rnn_hs</strong> (<em>torch.tensor</em>) – Policy recurrent hidden state obtained with the current ActorCritic version.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>loss_q1</strong> (<em>torch.tensor</em>) – Q1-net loss.</p></li>
<li><p><strong>loss_q2</strong> (<em>torch.tensor</em>) – Q2-net loss.</p></li>
<li><p><strong>loss_q</strong> (<em>torch.tensor</em>) – Weighted average of loss_q1 and loss_q2.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.create_factory">
<em class="property">classmethod </em><code class="sig-name descname">create_factory</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lr_q</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">lr_pi</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">lr_alpha</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">0.99</span></em>, <em class="sig-param"><span class="n">polyak</span><span class="o">=</span><span class="default_value">0.995</span></em>, <em class="sig-param"><span class="n">num_updates</span><span class="o">=</span><span class="default_value">50</span></em>, <em class="sig-param"><span class="n">test_every</span><span class="o">=</span><span class="default_value">5000</span></em>, <em class="sig-param"><span class="n">update_every</span><span class="o">=</span><span class="default_value">50</span></em>, <em class="sig-param"><span class="n">start_steps</span><span class="o">=</span><span class="default_value">20000</span></em>, <em class="sig-param"><span class="n">initial_alpha</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">mini_batch_size</span><span class="o">=</span><span class="default_value">64</span></em>, <em class="sig-param"><span class="n">reward_scaling</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">num_test_episodes</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">target_update_interval</span><span class="o">=</span><span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.create_factory" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a function to create new SAC instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr_pi</strong> (<em>float</em>) – Policy optimizer learning rate.</p></li>
<li><p><strong>lr_q</strong> (<em>float</em>) – Q-nets optimizer learning rate.</p></li>
<li><p><strong>lr_alpha</strong> (<em>float</em>) – Alpha optimizer learning rate.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – Discount factor parameter.</p></li>
<li><p><strong>initial_alpha</strong> (<em>float</em>) – Initial entropy coefficient value.</p></li>
<li><p><strong>polyak</strong> (<em>float</em>) – SAC polyak averaging parameter.</p></li>
<li><p><strong>num_updates</strong> (<em>int</em>) – Num consecutive actor_critic updates before data collection continues.</p></li>
<li><p><strong>update_every</strong> (<em>int</em>) – Regularity of actor_critic updates in number environment steps.</p></li>
<li><p><strong>start_steps</strong> (<em>int</em>) – Num of initial random environment steps before learning starts.</p></li>
<li><p><strong>mini_batch_size</strong> (<em>int</em>) – Size of actor_critic update batches.</p></li>
<li><p><strong>reward_scaling</strong> (<em>float</em>) – Reward scaling factor.</p></li>
<li><p><strong>target_update_interval</strong> (<em>float</em>) – regularity of target nets updates with respect to actor_critic Adam updates.</p></li>
<li><p><strong>num_test_episodes</strong> (<em>int</em>) – Number of episodes to complete in each test phase.</p></li>
<li><p><strong>test_every</strong> (<em>int</em>) – Regularity of test evaluations in actor_critic updates.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>create_algo_instance</strong> – creates a new SAC class instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>func</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.discrete_version">
<em class="property">property </em><code class="sig-name descname">discrete_version</code><a class="headerlink" href="#nappo.core.algos.sac.SAC.discrete_version" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if action_space is discrete.</p>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.set_weights">
<code class="sig-name descname">set_weights</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.set_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Update actor critic with the given weights. Update also target networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weights</strong> (<em>dict of tensors</em>) – Dict containing actor_critic weights to be set.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.update_algo_parameter">
<code class="sig-name descname">update_algo_parameter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parameter_name</span></em>, <em class="sig-param"><span class="n">new_parameter_value</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.update_algo_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>parameter_name</cite> is an attribute of the algorithm, change its value
to <cite>new_parameter_value value</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameter_name</strong> (<em>str</em>) – Worker.algo attribute name</p></li>
<li><p><strong>new_parameter_value</strong> (<em>int</em><em> or </em><em>float</em>) – New value for <cite>parameter_name</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nappo.core.algos.sac.SAC.update_target_networks">
<code class="sig-name descname">update_target_networks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nappo.core.algos.sac.SAC.update_target_networks" title="Permalink to this definition">¶</a></dt>
<dd><p>Update actor critic target networks with polyak averaging</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nappo.core.algos">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-nappo.core.algos" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">nappo.core.algos package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-nappo.core.algos.base">nappo.core.algos.base module</a></li>
<li><a class="reference internal" href="#module-nappo.core.algos.ppo">nappo.core.algos.ppo module</a></li>
<li><a class="reference internal" href="#module-nappo.core.algos.sac">nappo.core.algos.sac module</a></li>
<li><a class="reference internal" href="#module-nappo.core.algos">Module contents</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="nappo.core.actors.neural_networks.feature_extractors.html"
                        title="previous chapter">nappo.core.actors.neural_networks.feature_extractors package</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="nappo.core.envs.html"
                        title="next chapter">nappo.core.envs package</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/nappo.core.algos.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="nappo.core.envs.html" title="nappo.core.envs package"
             >next</a> |</li>
        <li class="right" >
          <a href="nappo.core.actors.neural_networks.feature_extractors.html" title="nappo.core.actors.neural_networks.feature_extractors package"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">nappo 0.0.22 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="nappo.html" >nappo package</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="nappo.core.html" >nappo.core package</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, nappo.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.3.
    </div>
  </body>
</html>